{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install opendatasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEsKDOxiYtzu",
        "outputId": "c9d20600-3059-4e3e-e82e-d136dc4af5f6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.12/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from opendatasets) (8.2.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.4.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I21xxragvfy-",
        "outputId": "283d641f-90d2-4ae3-bcbf-b3104a5d73f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./pokemon-sprites-images\" (use force=True to force download)\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "import os\n",
        "od.download(\"https://www.kaggle.com/datasets/yehongjiang/pokemon-sprites-images\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qkeml6NlviPN"
      },
      "outputs": [],
      "source": [
        "class PokemonDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "\n",
        "        for pokemon_name in os.listdir(root_dir):\n",
        "            front_normal_path = os.path.join(root_dir, pokemon_name, \"front\", \"normal\")\n",
        "            if os.path.exists(front_normal_path):\n",
        "                for file in os.listdir(front_normal_path):\n",
        "                    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
        "                        self.images.append(os.path.join(front_normal_path, file))\n",
        "                        break\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT_boFWvvovW",
        "outputId": "da8473ae-304b-4529-d1b3-2b5f6ef78235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0525-Combee-415', '0738-Lampent-608', '0170-Chansey-113', '1051-Toxtricity Low Key-849', '0736-Beheeyem-606']\n"
          ]
        }
      ],
      "source": [
        "print(os.listdir(\"./pokemon-sprites-images/pokemon_images/sprites\")[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wT0CMiC-YZ8I"
      },
      "outputs": [],
      "source": [
        "def sinusoidal_embedding(timesteps: torch.LongTensor, dim: int):\n",
        "    \"\"\"\n",
        "    timesteps: (B,) long tensor\n",
        "    returns: (B, dim) float tensor\n",
        "    \"\"\"\n",
        "    assert len(timesteps.shape) == 1\n",
        "    device = timesteps.device\n",
        "    half = dim // 2\n",
        "    freqs = torch.exp(-torch.log(torch.tensor(10000.0, device=device)) * torch.arange(half, device=device) / (half - 1))\n",
        "    args = timesteps.float().unsqueeze(1) * freqs.unsqueeze(0)\n",
        "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
        "    if dim % 2 == 1:\n",
        "        emb = F.pad(emb, (0, 1))\n",
        "    return emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "X2hgfDu1vqk9"
      },
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "    T.Resize((96, 96), interpolation=Image.NEAREST),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "dataset = PokemonDataset(root_dir=\"./pokemon-sprites-images/pokemon_images/sprites\", transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "X6o5KKVqYZ8I"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard convolutional block with GroupNorm, SiLU activation,\n",
        "    a residual connection, and time embedding injection.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch, time_emb_dim, is_res=True):\n",
        "        super().__init__()\n",
        "        self.is_res = is_res\n",
        "        self.main_path = nn.Sequential(\n",
        "            nn.GroupNorm(8, in_ch),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "        )\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_emb_dim, out_ch),\n",
        "        )\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = self.main_path(x)\n",
        "        time_emb = self.time_mlp(temb).unsqueeze(-1).unsqueeze(-1)\n",
        "        h = h + time_emb\n",
        "        return h + self.res_conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qpjpxkGNYZ8I"
      },
      "outputs": [],
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Self-attention block. Applies Multi-Head Self-Attention to 2D feature maps.\n",
        "    \"\"\"\n",
        "    def __init__(self, channels, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.num_heads = num_heads\n",
        "        self.norm = nn.GroupNorm(8, channels)\n",
        "        self.qkv = nn.Conv2d(channels, channels * 3, 1, bias=False)\n",
        "        self.proj = nn.Conv2d(channels, channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.norm(x)\n",
        "        qkv = self.qkv(x).reshape(B, 3, self.num_heads, C // self.num_heads, H * W)\n",
        "        q, k, v = qkv.unbind(1)\n",
        "\n",
        "        attn = torch.einsum('b h c i, b h c j -> b h i j', q, k) * ((C // self.num_heads) ** -0.5)\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        out = torch.einsum('b h i j, b h c j -> b h c i', attn, v)\n",
        "        out = out.reshape(B, C, H, W)\n",
        "        return x + self.proj(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PtkB2Ft9wJ5V"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, img_channels=3, base_channels=128, time_emb_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(time_emb_dim, time_emb_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim),\n",
        "        )\n",
        "\n",
        "        ch_mults = (1, 2, 4, 8)\n",
        "        channels = [base_channels] + [base_channels * m for m in ch_mults]\n",
        "\n",
        "        self.init_conv = nn.Conv2d(img_channels, base_channels, 3, padding=1)\n",
        "\n",
        "        self.down_blocks = nn.ModuleList()\n",
        "        for i in range(len(ch_mults)):\n",
        "            in_ch = channels[i]\n",
        "            out_ch = channels[i+1]\n",
        "            self.down_blocks.append(nn.ModuleList([\n",
        "                ConvBlock(in_ch, out_ch, time_emb_dim),\n",
        "                AttentionBlock(out_ch) if i >= 2 else nn.Identity(),\n",
        "                nn.Conv2d(out_ch, out_ch, 4, stride=2, padding=1)\n",
        "            ]))\n",
        "\n",
        "        self.mid_block1 = ConvBlock(channels[-1], channels[-1], time_emb_dim)\n",
        "        self.mid_attn = AttentionBlock(channels[-1])\n",
        "        self.mid_block2 = ConvBlock(channels[-1], channels[-1], time_emb_dim)\n",
        "\n",
        "        self.up_blocks = nn.ModuleList()\n",
        "        for i in reversed(range(len(ch_mults))):\n",
        "            in_ch = channels[i+1]\n",
        "            out_ch = channels[i]\n",
        "\n",
        "            self.up_blocks.append(nn.ModuleList([\n",
        "                nn.ConvTranspose2d(in_ch, out_ch, 4, stride=2, padding=1),\n",
        "                ConvBlock(out_ch + in_ch, out_ch, time_emb_dim),\n",
        "                AttentionBlock(out_ch) if i >= 2 else nn.Identity(),\n",
        "            ]))\n",
        "\n",
        "        self.final_norm = nn.GroupNorm(8, base_channels)\n",
        "        self.final_act = nn.SiLU()\n",
        "        self.final_conv = nn.Conv2d(base_channels, img_channels, 1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        temb = sinusoidal_embedding(t, 256)\n",
        "        temb = self.time_mlp(temb)\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        skips = [x]\n",
        "        for block, attn, downsample in self.down_blocks:\n",
        "            x = block(x, temb)\n",
        "            x = attn(x)\n",
        "            skips.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        x = self.mid_block1(x, temb)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, temb)\n",
        "\n",
        "        for upsample, block, attn in self.up_blocks:\n",
        "            x = upsample(x)\n",
        "            skip = skips.pop()\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "            x = block(x, temb)\n",
        "            x = attn(x)\n",
        "\n",
        "        x = self.final_norm(x)\n",
        "        x = self.final_act(x)\n",
        "        return self.final_conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "JblbnsOcwOE1"
      },
      "outputs": [],
      "source": [
        "class Diffusion:\n",
        "    def __init__(self, model: nn.Module, img_size=96, device=\"cuda\", timesteps=256):\n",
        "        self.model = model\n",
        "        self.img_size = img_size\n",
        "        self.device = device\n",
        "        self.timesteps = timesteps\n",
        "\n",
        "        betas = torch.linspace(1e-4, 0.02, timesteps, device=device)\n",
        "        alphas = 1.0 - betas\n",
        "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "        self.betas = betas\n",
        "        self.alphas = alphas\n",
        "        self.alphas_cumprod = alphas_cumprod\n",
        "        self.alphas_cumprod_prev = torch.cat([torch.tensor([1.0], device=device), alphas_cumprod[:-1]])\n",
        "\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)\n",
        "        self.sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "\n",
        "    def q_sample(self, x0, t, noise=None):\n",
        "        \"\"\"\n",
        "        sample from q(x_t | x_0)\n",
        "        x0: (B,C,H,W)\n",
        "        t: (B,) long tensor\n",
        "        \"\"\"\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x0)\n",
        "        sqrt_alpha_cumprod_t = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
        "        sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
        "        return sqrt_alpha_cumprod_t * x0 + sqrt_one_minus_alpha_cumprod_t * noise\n",
        "\n",
        "    def training_step(self, x0):\n",
        "        \"\"\"\n",
        "        single batch training step: pick random t for each sample\n",
        "        returns MSE loss between predicted noise and real noise\n",
        "        \"\"\"\n",
        "        b = x0.size(0)\n",
        "        t = torch.randint(0, self.timesteps, (b,), device=self.device, dtype=torch.long)\n",
        "        noise = torch.randn_like(x0)\n",
        "        x_noisy = self.q_sample(x0, t, noise)\n",
        "        noise_pred = self.model(x_noisy, t)\n",
        "        loss = F.mse_loss(noise_pred, noise)\n",
        "        return loss\n",
        "\n",
        "    def p_mean_variance(self, x_t, t):\n",
        "        \"\"\"\n",
        "        computes posterior mean and variance for q(x_{t-1} | x_t, x0_pred)\n",
        "        x_t: (B,C,H,W)\n",
        "        t: scalar int or 0-d python int\n",
        "        returns: posterior_mean, posterior_variance (both tensors shape (B,C,H,W) for mean and (B,1,1,1) for var)\n",
        "        \"\"\"\n",
        "        B = x_t.shape[0]\n",
        "        device = x_t.device\n",
        "        t_tensor = torch.full((B,), t, device=device, dtype=torch.long)\n",
        "        eps_theta = self.model(x_t, t_tensor)\n",
        "\n",
        "        sqrt_alpha_hat_t = self.sqrt_alphas_cumprod[t]\n",
        "        sqrt_one_minus_alpha_hat_t = self.sqrt_one_minus_alphas_cumprod[t]\n",
        "        x0_pred = (x_t - sqrt_one_minus_alpha_hat_t * eps_theta) / sqrt_alpha_hat_t\n",
        "        x0_pred = torch.clamp(x0_pred, -1.0, 1.0)\n",
        "\n",
        "        alpha_t = self.alphas[t]\n",
        "        alpha_hat_t = self.alphas_cumprod[t]\n",
        "        alpha_hat_prev = self.alphas_cumprod_prev[t]\n",
        "        beta_t = self.betas[t]\n",
        "\n",
        "        coef_x0 = (torch.sqrt(alpha_hat_prev) * beta_t) / (1.0 - alpha_hat_t)\n",
        "        coef_xt = (torch.sqrt(alpha_t) * (1.0 - alpha_hat_prev)) / (1.0 - alpha_hat_t)\n",
        "\n",
        "        coef_x0 = coef_x0.view(1, 1, 1, 1)\n",
        "        coef_xt = coef_xt.view(1, 1, 1, 1)\n",
        "\n",
        "        posterior_mean = coef_x0 * x0_pred + coef_xt * x_t\n",
        "        posterior_variance = beta_t * (1.0 - alpha_hat_prev) / (1.0 - alpha_hat_t)\n",
        "        posterior_log_variance = torch.log(torch.clamp(posterior_variance, min=1e-20)).view(1, 1, 1, 1)\n",
        "\n",
        "        return posterior_mean, posterior_variance, posterior_log_variance\n",
        "\n",
        "    def p_sample(self, x_t, t):\n",
        "        \"\"\"\n",
        "        sample x_{t-1} from p(x_{t-1} | x_t)\n",
        "        \"\"\"\n",
        "        mean, var, log_var = self.p_mean_variance(x_t, t)\n",
        "        if t == 0:\n",
        "            return mean\n",
        "        noise = torch.randn_like(x_t)\n",
        "        return mean + torch.sqrt(var).view(1,1,1,1) * noise\n",
        "\n",
        "    def sample(self, batch_size=8):\n",
        "        \"\"\"\n",
        "        Full sampling loop: start from x_T ~ N(0,I), run p_sample iteratively.\n",
        "        \"\"\"\n",
        "        x = torch.randn(batch_size, 3, self.img_size, self.img_size, device=self.device)\n",
        "        for t in reversed(range(self.timesteps)):\n",
        "            x = self.p_sample(x, t)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WehbqmXaYZ8J"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = UNet(img_channels=3, base_channels=128, time_emb_dim=256).to(device)\n",
        "diffusion = Diffusion(model, img_size=96, device=device, timesteps=1000)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "id": "hKVzXjJjYZ8J",
        "outputId": "54089fe2-c4c5-42f2-9f73-f47745df666e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.8117647..1.0].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.0000, -0.9922, -0.9373, -0.9216, -0.8745, -0.8510, -0.8275, -0.8196,\n",
            "        -0.8118, -0.8039, -0.7882, -0.7804, -0.7725, -0.7569, -0.7490, -0.7412,\n",
            "        -0.7333, -0.7176, -0.7098, -0.7020, -0.6941, -0.6863, -0.6784, -0.6627,\n",
            "        -0.6549, -0.6471, -0.6392, -0.6314, -0.6235, -0.6157, -0.6078, -0.6000,\n",
            "        -0.5922, -0.5765, -0.5686, -0.5608, -0.5529, -0.5451, -0.5216, -0.5059,\n",
            "        -0.4980, -0.4902, -0.4824, -0.4745, -0.4667, -0.4510, -0.4431, -0.4353,\n",
            "        -0.4275, -0.4196, -0.3961, -0.3804, -0.3725, -0.3569, -0.3490, -0.3412,\n",
            "        -0.3255, -0.3176, -0.3098, -0.3020, -0.2941, -0.2863, -0.2784, -0.2627,\n",
            "        -0.2549, -0.2471, -0.2392, -0.2314, -0.2235, -0.2078, -0.1922, -0.1843,\n",
            "        -0.1765, -0.1686, -0.1608, -0.1294, -0.1216, -0.1137, -0.0980, -0.0745,\n",
            "        -0.0588, -0.0353, -0.0275, -0.0118, -0.0039,  0.0039,  0.0118,  0.0275,\n",
            "         0.0353,  0.0431,  0.0588,  0.0667,  0.0745,  0.0824,  0.0902,  0.0980,\n",
            "         0.1137,  0.1294,  0.1373,  0.1451,  0.1529,  0.1608,  0.1765,  0.1922,\n",
            "         0.2000,  0.2078,  0.2157,  0.2235,  0.2392,  0.2471,  0.2549,  0.2863,\n",
            "         0.2941,  0.3176,  0.3333,  0.3412,  0.3490,  0.3569,  0.3725,  0.3804,\n",
            "         0.3961,  0.4039,  0.4118,  0.4196,  0.4275,  0.4431,  0.4667,  0.4824,\n",
            "         0.4902,  0.4980,  0.5059,  0.5216,  0.5294,  0.5451,  0.5686,  0.5765,\n",
            "         0.5843,  0.5922,  0.6078,  0.6157,  0.6235,  0.6314,  0.6392,  0.6471,\n",
            "         0.6549,  0.6706,  0.6784,  0.6941,  0.7020,  0.7098,  0.7255,  0.7412,\n",
            "         0.7490,  0.7569,  0.8039,  0.8118,  0.8196,  0.8275,  0.8510,  0.8588,\n",
            "         0.8667,  0.8745,  0.8824,  0.8980,  0.9059,  0.9216,  0.9294,  0.9451,\n",
            "         0.9686,  0.9765,  0.9843,  0.9922,  1.0000])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "img, _ = next(iter(dataloader))\n",
        "print(torch.unique(img))\n",
        "plt.imshow(img[0].permute(1, 2, 0).cpu().numpy())\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsw0bLLVwQ01",
        "outputId": "c477f53d-005c-4fd0-b6df-00bf8db41b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3500: 100%|██████████| 34/34 [00:28<00:00,  1.20it/s, loss=0.0608, iters=34]\n",
            "Epoch 2/3500: 100%|██████████| 34/34 [00:27<00:00,  1.24it/s, loss=0.1, iters=68]\n",
            "Epoch 3/3500: 100%|██████████| 34/34 [00:27<00:00,  1.23it/s, loss=0.0213, iters=102]\n",
            "Epoch 4/3500: 100%|██████████| 34/34 [00:28<00:00,  1.20it/s, loss=0.0299, iters=136]\n",
            "Epoch 5/3500: 100%|██████████| 34/34 [00:28<00:00,  1.18it/s, loss=0.0449, iters=170]\n",
            "Epoch 6/3500: 100%|██████████| 34/34 [00:29<00:00,  1.16it/s, loss=0.0247, iters=204]\n",
            "Epoch 7/3500: 100%|██████████| 34/34 [00:29<00:00,  1.16it/s, loss=0.0486, iters=238]\n",
            "Epoch 8/3500: 100%|██████████| 34/34 [00:29<00:00,  1.14it/s, loss=0.0244, iters=272]\n",
            "Epoch 9/3500: 100%|██████████| 34/34 [00:29<00:00,  1.14it/s, loss=0.0242, iters=306]\n",
            "Epoch 10/3500: 100%|██████████| 34/34 [00:30<00:00,  1.13it/s, loss=0.025, iters=340]\n",
            "Epoch 11/3500: 100%|██████████| 34/34 [00:30<00:00,  1.12it/s, loss=0.0656, iters=374]\n",
            "Epoch 12/3500: 100%|██████████| 34/34 [00:30<00:00,  1.12it/s, loss=0.0281, iters=408]\n",
            "Epoch 13/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0425, iters=442]\n",
            "Epoch 14/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0351, iters=476]\n",
            "Epoch 15/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0788, iters=510]\n",
            "Epoch 16/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0577, iters=544]\n",
            "Epoch 17/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0182, iters=578]\n",
            "Epoch 18/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0335, iters=612]\n",
            "Epoch 19/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0242, iters=646]\n",
            "Epoch 20/3500: 100%|██████████| 34/34 [00:30<00:00,  1.10it/s, loss=0.0845, iters=680]\n",
            "Epoch 21/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0266, iters=714]\n",
            "Epoch 22/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.042, iters=748]\n",
            "Epoch 23/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0347, iters=782]\n",
            "Epoch 24/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0295, iters=816]\n",
            "Epoch 25/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0288, iters=850]\n",
            "Epoch 26/3500: 100%|██████████| 34/34 [00:30<00:00,  1.10it/s, loss=0.0164, iters=884]\n",
            "Epoch 27/3500: 100%|██████████| 34/34 [00:30<00:00,  1.10it/s, loss=0.0135, iters=918]\n",
            "Epoch 28/3500: 100%|██████████| 34/34 [00:30<00:00,  1.10it/s, loss=0.0238, iters=952]\n",
            "Epoch 29/3500: 100%|██████████| 34/34 [00:30<00:00,  1.10it/s, loss=0.0146, iters=986]\n",
            "Epoch 30/3500: 100%|██████████| 34/34 [00:30<00:00,  1.10it/s, loss=0.0225, iters=1020]\n",
            "Epoch 31/3500: 100%|██████████| 34/34 [00:30<00:00,  1.10it/s, loss=0.0255, iters=1054]\n",
            "Epoch 32/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0129, iters=1088]\n",
            "Epoch 33/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0244, iters=1122]\n",
            "Epoch 34/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.035, iters=1156]\n",
            "Epoch 35/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0496, iters=1190]\n",
            "Epoch 36/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0218, iters=1224]\n",
            "Epoch 37/3500: 100%|██████████| 34/34 [00:30<00:00,  1.10it/s, loss=0.0288, iters=1258]\n",
            "Epoch 38/3500: 100%|██████████| 34/34 [00:30<00:00,  1.10it/s, loss=0.0144, iters=1292]\n",
            "Epoch 39/3500: 100%|██████████| 34/34 [00:30<00:00,  1.10it/s, loss=0.0365, iters=1326]\n",
            "Epoch 40/3500: 100%|██████████| 34/34 [00:30<00:00,  1.10it/s, loss=0.0292, iters=1360]\n",
            "Epoch 41/3500: 100%|██████████| 34/34 [00:30<00:00,  1.10it/s, loss=0.067, iters=1394]\n",
            "Epoch 42/3500: 100%|██████████| 34/34 [00:30<00:00,  1.10it/s, loss=0.02, iters=1428]\n",
            "Epoch 43/3500: 100%|██████████| 34/34 [00:31<00:00,  1.10it/s, loss=0.0149, iters=1462]\n",
            "Epoch 44/3500: 100%|██████████| 34/34 [00:30<00:00,  1.10it/s, loss=0.00906, iters=1496]\n",
            "Epoch 45/3500: 100%|██████████| 34/34 [00:30<00:00,  1.10it/s, loss=0.0163, iters=1530]\n",
            "Epoch 46/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0244, iters=1564]\n",
            "Epoch 47/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0294, iters=1598]\n",
            "Epoch 48/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0134, iters=1632]\n",
            "Epoch 49/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0108, iters=1666]\n",
            "Epoch 50/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.0348, iters=1700]\n",
            "Epoch 51/3500: 100%|██████████| 34/34 [00:30<00:00,  1.11it/s, loss=0.00213, iters=1734]\n",
            "Epoch 52/3500:  12%|█▏        | 4/34 [00:03<00:27,  1.09it/s, loss=0.0273, iters=1738]"
          ]
        }
      ],
      "source": [
        "num_epochs = 3500\n",
        "iters = 0\n",
        "for epoch in range(num_epochs):\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for imgs, _ in pbar:\n",
        "        imgs = imgs.to(device)\n",
        "        loss = diffusion.training_step(imgs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        iters += 1\n",
        "        pbar.set_postfix({\"loss\": loss.item(), \"iters\": iters})\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            samples = diffusion.sample(batch_size=8)\n",
        "            samples = (samples * 0.5 + 0.5).clamp(0, 1)\n",
        "        model.train()\n",
        "\n",
        "        fig, axes = plt.subplots(1, 8, figsize=(16, 2))\n",
        "        for i in range(8):\n",
        "            img = samples[i].permute(1, 2, 0).cpu().numpy()\n",
        "            axes[i].imshow(img)\n",
        "            axes[i].axis(\"off\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjikKjJIYZ8K"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    samples = diffusion.sample(batch_size=8)\n",
        "    fig, axes = plt.subplots(1, 8, figsize=(16, 2))\n",
        "    for i in range(8):\n",
        "        axes[i].imshow(samples[i].permute(1, 2, 0).cpu().numpy())\n",
        "        axes[i].axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3dDgXwWYZ8K"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"unet_pokemon_diffusion.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}